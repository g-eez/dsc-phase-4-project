{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5fda70",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Twitter Sentiment Analysis: A Multi-Stage NLP Project\n",
    "\n",
    "This project focuses on building a robust Natural Language Processing (NLP) model to classify the sentiment of tweets about Apple and Google products. We'll approach this as an iterative process, starting with a simple binary classification and advancing to a more complex multiclass solution.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Business Problem & Project Goals** üéØ\n",
    "\n",
    "Our primary goal is to create an automated system that can quickly and accurately gauge public opinion about major tech brands from social media data. The delivered model will enable businesses to monitor brand perception in real-time and make **data-driven decisions** in marketing, product development, and public relations.\n",
    "\n",
    "**Key Deliverables:**\n",
    "\n",
    "* A well-documented, reproducible NLP pipeline.\n",
    "* A binary classification model (Positive vs. Negative).\n",
    "* A multiclass classification model (Positive, Negative, and Neutral).\n",
    "* A clear analysis of model performance and business-relevant insights.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Data Preparation & Cleaning** üßπ\n",
    "\n",
    "Raw text data is inherently messy and requires careful cleaning before it can be used for modeling. The cleaning process transforms noise into valuable features.\n",
    "\n",
    "**Core Cleaning Steps:**\n",
    "\n",
    "1.  **Standardization:** Convert all text to lowercase to ensure uniformity.\n",
    "2.  **Noise Removal:** Use regular expressions to strip out irrelevant elements like URLs, user mentions (`@username`), and hashtags (`#`).\n",
    "3.  **Tokenization:** Break down the cleaned text into individual words or \"tokens.\"\n",
    "4.  **Stemming or Lemmatization:** Reduce words to their root form. **Lemmatization** is generally preferred as it converts words to their meaningful base form (e.g., \"running\" becomes \"run\"), which preserves more semantic context than stemming.\n",
    "5.  **Stop Word Removal:** Eliminate common, low-information words (e.g., \"the,\" \"is\") that do not contribute to sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Feature Engineering** üõ†Ô∏è\n",
    "\n",
    "Machine learning models require numerical input. This step is about converting our cleaned text tokens into a numerical representation.\n",
    "\n",
    "**Vectorization Techniques:**\n",
    "\n",
    "* **TF-IDF (Term Frequency-Inverse Document Frequency):** This is a highly effective method for text classification. It assigns a numerical score to each word that reflects its importance in a single tweet relative to the entire dataset. This method helps to down-weigh common words and highlight unique, important words that are crucial for sentiment detection.\n",
    "* **N-grams:** Beyond single words, we will consider using n-grams (e.g., bigrams, trigrams). This captures the sentiment of multi-word phrases like **\"not good\"** or **\"loved it,\"** which a simple bag-of-words approach would miss.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Model Building: From Binary to Multiclass** üìà\n",
    "\n",
    "This is the central part of the project, where we progressively build and refine our models.\n",
    "\n",
    "#### **Stage 1: Binary Classification (Positive vs. Negative)**\n",
    "\n",
    "* **Data Subset:** Filter the dataset to include only \"positive\" and \"negative\" tweets.\n",
    "* **Model Selection:** We will start with a simple yet powerful algorithm, such as **Logistic Regression** or **Naive Bayes**. These models are excellent baselines for text classification and are highly interpretable.\n",
    "* **Pipeline Creation:** We'll use a `scikit-learn` pipeline to combine our TF-IDF vectorizer and our classifier. This ensures a clean workflow and prevents data leakage.\n",
    "\n",
    "#### **Stage 2: Multiclass Classification (Positive, Negative, and Neutral)**\n",
    "\n",
    "* **Data Extension:** Re-introduce the \"neutral\" tweets to the dataset.\n",
    "* **Model Adaptation:** The same classification algorithms can be used for multiclass problems.\n",
    "* **Evaluation Focus:** Multiclass problems are more complex. While accuracy gives a general idea of performance, it's not enough.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Evaluation & Business-Informed Metrics** üìä\n",
    "\n",
    "Evaluation is the most critical stage. The choice of metrics must be guided by the business problem.\n",
    "\n",
    "#### **Choosing the Right Metrics:**\n",
    "\n",
    "* **F1-Score (Macro Average):** This is the key metric for your project. A **macro-averaged F1-score** calculates the F1-score for each class independently and then takes the average. This is vital for imbalanced datasets because it gives equal weight to all classes, preventing the model from performing well on a majority class while ignoring a minority class. This aligns with our business goal of understanding sentiment across all three categories, not just the most common one.\n",
    "* **Confusion Matrix:** This visual tool will show us exactly where our model is making mistakes (e.g., misclassifying a \"negative\" tweet as \"neutral\"). This helps us understand which misclassifications are most problematic. For example, misclassifying a negative review as positive is far more costly than classifying it as neutral.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Insights & Conclusion** üìù\n",
    "\n",
    "The final phase involves interpreting our model's performance and translating it into actionable business intelligence. We'll analyze common terms and phrases associated with each sentiment and provide concrete recommendations for Apple and Google based on our findings. The ultimate deliverable is a proof-of-concept that demonstrates the power of NLP for real-time sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf7e6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sklearn - ML\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Persistence\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c2bee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (9093, 3)\n",
      "\n",
      "Columns: ['tweet_text', 'emotion_in_tweet_is_directed_at', 'is_there_an_emotion_directed_at_a_brand_or_product']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tweet_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "emotion_in_tweet_is_directed_at",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "is_there_an_emotion_directed_at_a_brand_or_product",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "84382f90-051d-4d1b-a1c8-21163e3b0a98",
       "rows": [
        [
         "0",
         ".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.",
         "iPhone",
         "Negative emotion"
        ],
        [
         "1",
         "@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW",
         "iPad or iPhone App",
         "Positive emotion"
        ],
        [
         "2",
         "@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.",
         "iPad",
         "Positive emotion"
        ],
        [
         "3",
         "@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw",
         "iPad or iPhone App",
         "Negative emotion"
        ],
        [
         "4",
         "@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)",
         "Google",
         "Positive emotion"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "file_path = r\"data\\judge-1377884607_tweet_product_company.csv\"\n",
    "\n",
    "# Try ISO-8859-1 (common fallback for these datasets)\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Basic dataset info\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbee34",
   "metadata": {},
   "source": [
    "Perfect ‚úÖ The dataset has **9,093 tweets** and the 3 key columns we expected:\n",
    "\n",
    "* **`tweet_text`** ‚Üí the actual tweet content (our input features).\n",
    "* **`emotion_in_tweet_is_directed_at`** ‚Üí the entity/brand mentioned (Apple, Google, iPhone, etc.).\n",
    "* **`is_there_an_emotion_directed_at_a_brand_or_product`** ‚Üí the sentiment label (Positive, Negative, Neutral, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Step 2: Focus on Relevant Brands (Apple & Google)\n",
    "\n",
    "### üîé Explanation\n",
    "\n",
    "* The dataset covers multiple products/brands, but our project goal is **Apple vs Google**.\n",
    "* We need to:\n",
    "\n",
    "  1. **Map related terms** (e.g., \"iPhone\", \"iPad\", \"MacBook\" ‚Üí Apple; \"Android\", \"Nexus\" ‚Üí Google).\n",
    "  2. Keep only those rows.\n",
    "  3. Normalize the `is_there_an_emotion_directed_at_a_brand_or_product` column into simpler labels:\n",
    "\n",
    "     * `positive`\n",
    "     * `negative`\n",
    "     * `neutral`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66545572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after entity + text detection: (8338, 4)\n",
      "\n",
      "Sentiment counts:\n",
      "sentiment\n",
      "neutral     4804\n",
      "positive    2965\n",
      "negative     569\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tweet_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "emotion_in_tweet_is_directed_at",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "is_there_an_emotion_directed_at_a_brand_or_product",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "brand",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "524eab4c-d74b-4589-a2d3-a4b65050bd18",
       "rows": [
        [
         "0",
         ".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.",
         "iPhone",
         "Negative emotion",
         "Apple",
         "negative"
        ],
        [
         "1",
         "@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW",
         "iPad or iPhone App",
         "Positive emotion",
         "Apple",
         "positive"
        ],
        [
         "2",
         "@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.",
         "iPad",
         "Positive emotion",
         "Apple",
         "positive"
        ],
        [
         "3",
         "@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw",
         "iPad or iPhone App",
         "Negative emotion",
         "Apple",
         "negative"
        ],
        [
         "4",
         "@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)",
         "Google",
         "Positive emotion",
         "Google",
         "positive"
        ],
        [
         "5",
         "@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd",
         null,
         "No emotion toward brand or product",
         "Apple",
         "neutral"
        ],
        [
         "7",
         "#SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan",
         "Android",
         "Positive emotion",
         "Google",
         "positive"
        ],
        [
         "8",
         "Beautifully smart and simple idea RT @madebymany @thenextweb wrote about our #hollergram iPad app for #sxsw! http://bit.ly/ieaVOB",
         "iPad or iPhone App",
         "Positive emotion",
         "Apple",
         "positive"
        ],
        [
         "9",
         "Counting down the days to #sxsw plus strong Canadian dollar means stock up on Apple gear",
         "Apple",
         "Positive emotion",
         "Apple",
         "positive"
        ],
        [
         "10",
         "Excited to meet the @samsungmobileus at #sxsw so I can show them my Sprint Galaxy S still running Android 2.1.   #fail",
         "Android",
         "Positive emotion",
         "Google",
         "positive"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>brand</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>Apple</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Apple</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw ...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Google</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "0   .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1   @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2   @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3   @sxsw I hope this year's festival isn't as cra...   \n",
       "4   @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "5   @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "7   #SXSW is just starting, #CTIA is around the co...   \n",
       "8   Beautifully smart and simple idea RT @madebyma...   \n",
       "9   Counting down the days to #sxsw plus strong Ca...   \n",
       "10  Excited to meet the @samsungmobileus at #sxsw ...   \n",
       "\n",
       "   emotion_in_tweet_is_directed_at  \\\n",
       "0                           iPhone   \n",
       "1               iPad or iPhone App   \n",
       "2                             iPad   \n",
       "3               iPad or iPhone App   \n",
       "4                           Google   \n",
       "5                              NaN   \n",
       "7                          Android   \n",
       "8               iPad or iPhone App   \n",
       "9                            Apple   \n",
       "10                         Android   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product   brand sentiment  \n",
       "0                                    Negative emotion   Apple  negative  \n",
       "1                                    Positive emotion   Apple  positive  \n",
       "2                                    Positive emotion   Apple  positive  \n",
       "3                                    Negative emotion   Apple  negative  \n",
       "4                                    Positive emotion  Google  positive  \n",
       "5                  No emotion toward brand or product   Apple   neutral  \n",
       "7                                    Positive emotion  Google  positive  \n",
       "8                                    Positive emotion   Apple  positive  \n",
       "9                                    Positive emotion   Apple  positive  \n",
       "10                                   Positive emotion  Google  positive  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start fresh\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Mapping from entity column\n",
    "apple_entities = [\n",
    "    \"Apple\", \"iPhone\", \"iPad\", \"iPad or iPhone App\", \"Other Apple product or service\"\n",
    "]\n",
    "google_entities = [\n",
    "    \"Google\", \"Android\", \"Android App\", \"Other Google product or service\"\n",
    "]\n",
    "\n",
    "def map_brand_from_entity(entity):\n",
    "    if pd.isna(entity):\n",
    "        return None\n",
    "    if entity in apple_entities:\n",
    "        return \"Apple\"\n",
    "    elif entity in google_entities:\n",
    "        return \"Google\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_clean[\"brand\"] = df_clean[\"emotion_in_tweet_is_directed_at\"].apply(map_brand_from_entity)\n",
    "\n",
    "# --- Fallback: detect brand in tweet text ---\n",
    "apple_keywords = [\"apple\", \"iphone\", \"ipad\", \"mac\", \"ipod\", \"ios\", \"imac\", \"macbook\"]\n",
    "google_keywords = [\"google\", \"android\", \"nexus\", \"pixel\", \"gmail\", \"youtube\", \"chrome\"]\n",
    "\n",
    "def detect_brand_from_text(text):\n",
    "    text = str(text).lower()\n",
    "    for kw in apple_keywords:\n",
    "        if kw in text:\n",
    "            return \"Apple\"\n",
    "    for kw in google_keywords:\n",
    "        if kw in text:\n",
    "            return \"Google\"\n",
    "    return None\n",
    "\n",
    "# Fill missing brands from tweet text\n",
    "df_clean.loc[df_clean[\"brand\"].isna(), \"brand\"] = df_clean.loc[\n",
    "    df_clean[\"brand\"].isna(), \"tweet_text\"\n",
    "].apply(detect_brand_from_text)\n",
    "\n",
    "# Drop rows that still have no brand\n",
    "df_clean = df_clean.dropna(subset=[\"brand\"])\n",
    "\n",
    "print(\"Remaining rows after entity + text detection:\", df_clean.shape)\n",
    "\n",
    "# Normalize sentiment labels\n",
    "def normalize_sentiment(label):\n",
    "    if \"Positive\" in label:\n",
    "        return \"positive\"\n",
    "    elif \"Negative\" in label:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df_clean[\"sentiment\"] = df_clean[\n",
    "    \"is_there_an_emotion_directed_at_a_brand_or_product\"\n",
    "].apply(normalize_sentiment)\n",
    "\n",
    "# Sentiment distribution check\n",
    "print(\"\\nSentiment counts:\")\n",
    "print(df_clean[\"sentiment\"].value_counts())\n",
    "\n",
    "df_clean.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931fdd4",
   "metadata": {},
   "source": [
    "\n",
    "### ‚úÖ What we have so far:\n",
    "\n",
    "* Input text: **`tweet_text`**\n",
    "* Target brand: **`brand`** (Apple / Google)\n",
    "* Sentiment label: **`sentiment`** (positive / negative / neutral)\n",
    "\n",
    "Class distribution:\n",
    "\n",
    "* **Neutral:** 4,804\n",
    "* **Positive:** 2,965\n",
    "* **Negative:** 569\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Text Preprocessing & Tokenization** üßπ\n",
    "\n",
    "Cleaning and tokenizing text is the foundation of any NLP pipeline. The goal is to transform raw, noisy tweets into structured tokens that preserve sentiment while removing irrelevant details.\n",
    "\n",
    "### **Challenges in Sentiment Tweets**\n",
    "\n",
    "* **Negations**: Phrases like *‚Äúcan not wait‚Äù* must be preserved as **‚Äúnot\\_wait‚Äù**; otherwise, the sentiment flips.\n",
    "* **Product Identifiers**: Phrases like *‚ÄúiPhone 3G‚Äù* or *‚ÄúiPad 2‚Äù* are meaningful and shouldn‚Äôt be stripped away.\n",
    "* **Noise**: Hashtags, mentions, and punctuation often don‚Äôt add value and can safely be removed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Our Tokenization Approach** üéØ\n",
    "\n",
    "We designed a **custom word-level tokenizer** that:\n",
    "\n",
    "1. Converts text to lowercase for consistency.\n",
    "2. Removes URLs, mentions (`@username`), and hashtags (`#`).\n",
    "3. Preserves **negation phrases** as single tokens (e.g., `\"can't wait\"` ‚Üí `not_wait`).\n",
    "4. Removes stopwords while **keeping negations**.\n",
    "5. Lemmatizes tokens to their root form (`stations` ‚Üí `station`).\n",
    "6. Keeps product numbers and meaningful terms (`iPad 2`, `iPhone 3G`).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e6194d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 (original): .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead! I can't wait to upgrade.\n",
      "Example 1 (processed): ['3g', 'iphone', 'hr', 'tweeting', 'wa', 'dead', 'not_wait', 'upgrade']\n",
      "\n",
      "Example 2 (original): @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.\n",
      "Example 2 (processed): ['not_wait', 'ipad', 'also', 'sale', 'sxsw']\n"
     ]
    }
   ],
   "source": [
    "# Regex pattern: allow words with letters, numbers, and apostrophes\n",
    "token_pattern = re.compile(r\"\\b[a-zA-Z0-9][a-zA-Z0-9']*\\b\")\n",
    "\n",
    "# Initialize the lemmatizer outside the function for efficiency\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define stop words, explicitly removing negation words\n",
    "stop_words = set(stopwords.words(\"english\")) - {\"not\", \"no\", \"nor\", \"n't\"}\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # 1. Lowercase text\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # 2. Remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    \n",
    "    # 3. Handle contractions (e.g., can't -> can not)\n",
    "    # This is important for separating \"n't\"\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "\n",
    "    # 4. Tokenize\n",
    "    # Your original token_pattern is good for finding words with apostrophes\n",
    "    token_pattern = re.compile(r\"\\b[a-zA-Z0-9][a-zA-Z0-9']*\\b\")\n",
    "    tokens = token_pattern.findall(text)\n",
    "\n",
    "    # 5. Handle negations: join \"not\" with the next word\n",
    "    processed_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] in [\"not\", \"no\", \"nor\"] and i + 1 < len(tokens):\n",
    "            processed_tokens.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            i += 2\n",
    "        else:\n",
    "            processed_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "\n",
    "    # 6. Lemmatize and remove stopwords\n",
    "    final_tokens = []\n",
    "    for t in processed_tokens:\n",
    "        # Lemmatize the token\n",
    "        t_lemmatized = lemmatizer.lemmatize(t)\n",
    "        \n",
    "        # Check if the lemmatized token should be removed\n",
    "        if t_lemmatized not in stop_words and len(t_lemmatized) > 1:\n",
    "            final_tokens.append(t_lemmatized)\n",
    "            \n",
    "    return final_tokens\n",
    "\n",
    "# Test the improved function with your examples\n",
    "ex1 = \".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead! I can't wait to upgrade.\"\n",
    "ex2 = \"@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.\"\n",
    "\n",
    "print(\"Example 1 (original):\", ex1)\n",
    "print(\"Example 1 (processed):\", custom_tokenizer(ex1))\n",
    "print(\"\\nExample 2 (original):\", ex2)\n",
    "print(\"Example 2 (processed):\", custom_tokenizer(ex2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a18f11",
   "metadata": {},
   "source": [
    "\n",
    "## **4. Feature Engineering: TF-IDF + N-Grams** üõ†Ô∏è\n",
    "\n",
    "Now that we have a clean, sentiment-preserving tokenizer, the next step is to convert our tokens into numerical features that a machine learning model can understand.\n",
    "\n",
    "### **Why TF-IDF?**\n",
    "\n",
    "* **TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)** assigns higher weights to words that are frequent in a document but rare across the dataset.\n",
    "* This helps downweight overly common words (like *‚Äúphone‚Äù*) and highlight more informative ones (like *‚Äúnot\\_wait‚Äù* or *‚Äúlove‚Äù*).\n",
    "\n",
    "### **Why N-Grams?**\n",
    "\n",
    "* **Unigrams**: Capture individual words (`good`, `bad`, `upgrade`).\n",
    "* **Bigrams**: Capture short phrases (`not_good`, `very_happy`) that often carry sentiment more strongly than single words.\n",
    "* Using both (n=1,2) balances **breadth (unigrams)** with **context (bigrams)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation Plan**\n",
    "\n",
    "1. Use **scikit-learn‚Äôs `TfidfVectorizer`**.\n",
    "2. Pass in our **custom tokenizer** (from Step 3).\n",
    "3. Limit to **max\\_features=5000** for efficiency.\n",
    "4. Use **ngram\\_range=(1,2)** to capture unigrams + bigrams.\n",
    "5. Generate the TF-IDF **feature matrix `X`** and the **labels `y`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3db900e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (8338, 5000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define TF-IDF vectorizer with custom tokenizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,   # from Step 3\n",
    "    ngram_range=(1, 2),           # unigrams + bigrams\n",
    "    max_features=5000             # cap vocabulary size\n",
    ")\n",
    "\n",
    "# Transform text into TF-IDF matrix\n",
    "X_tfidf = tfidf.fit_transform(df_clean[\"tweet_text\"])\n",
    "y = df_clean[\"sentiment\"]\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe9270",
   "metadata": {},
   "source": [
    "\n",
    "* **8338 rows** = each tweet in your cleaned dataset\n",
    "* **5000 columns** = top 5,000 unigrams + bigrams from TF-IDF\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 4 Summary\n",
    "\n",
    "We now have:\n",
    "\n",
    "* A **cleaned, tokenized dataset** that preserves sentiment signals.\n",
    "* A **TF-IDF feature matrix** ready for machine learning.\n",
    "* Labels (`y`) mapped to **positive, negative, neutral** sentiment.\n",
    "\n",
    "This sets us up for the **modeling stage**.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Step 5: Model Building (Binary ‚Üí Multiclass)\n",
    "\n",
    "We‚Äôll approach this in **two stages**:\n",
    "\n",
    "1. **Binary classification** (Positive vs Negative) ‚Äî to build a strong baseline.\n",
    "2. **Multiclass classification** (Positive, Negative, Neutral) ‚Äî to tackle the full business problem.\n",
    "\n",
    "We‚Äôll use **Logistic Regression** as the first model because:\n",
    "\n",
    "* It‚Äôs fast and interpretable.\n",
    "* A strong baseline for text classification.\n",
    "* Works well with TF-IDF features.\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Plan for Stage 1 (Binary)\n",
    "\n",
    "1. Filter dataset to only **positive** and **negative** tweets.\n",
    "2. Split into **train/test** sets.\n",
    "3. Build a **pipeline**:\n",
    "\n",
    "   * `TfidfVectorizer` (we already fit one, but in pipeline we‚Äôll rebuild for clean workflow).\n",
    "   * `LogisticRegression` classifier.\n",
    "4. Train and evaluate using:\n",
    "\n",
    "   * **Accuracy**\n",
    "   * **F1-score (macro)**\n",
    "   * **Confusion matrix**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93a8768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Binary):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.65      0.56       114\n",
      "    positive       0.93      0.87      0.90       593\n",
      "\n",
      "    accuracy                           0.84       707\n",
      "   macro avg       0.71      0.76      0.73       707\n",
      "weighted avg       0.86      0.84      0.84       707\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 74  40]\n",
      " [ 76 517]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter dataset for binary classification\n",
    "df_binary = df_clean[df_clean[\"sentiment\"].isin([\"positive\", \"negative\"])]\n",
    "\n",
    "X_bin = df_binary[\"tweet_text\"]\n",
    "y_bin = df_binary[\"sentiment\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_bin, y_bin, test_size=0.2, random_state=42, stratify=y_bin\n",
    ")\n",
    "\n",
    "# Build pipeline: TF-IDF + Logistic Regression\n",
    "pipeline_bin = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        tokenizer=custom_tokenizer, \n",
    "        ngram_range=(1,2),\n",
    "        max_features=5000\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline_bin.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline_bin.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report (Binary):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf52fee",
   "metadata": {},
   "source": [
    "\n",
    "## **Stage 1: Binary Classification Results (Positive vs Negative)**\n",
    "\n",
    "### **1. Classification Report**\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "negative       0.48       0.65      0.55       114\n",
    "positive       0.93       0.87      0.90       593\n",
    "accuracy                             0.83       707\n",
    "macro avg      0.71       0.76      0.73       707\n",
    "weighted avg   0.86       0.83      0.84       707\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* **Positive tweets**: Excellent performance ‚Äî precision (0.93) and recall (0.87) are very strong.\n",
    "* **Negative tweets**: Weak performance ‚Äî recall (0.65) is decent (it catches most negatives), but precision (0.48) is low ‚Üí meaning many positives are incorrectly classified as negative.\n",
    "* **Overall Accuracy = 83%**, but since the dataset is **imbalanced** (more positive than negative), accuracy is misleading.\n",
    "* **Macro F1 = 0.73** is a better reflection ‚Äî performance across both classes is only moderate.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Confusion Matrix**\n",
    "\n",
    "```\n",
    "[[ 74  40]   # negative: 74 correct, 40 misclassified as positive\n",
    " [ 79 514]]  # positive: 514 correct, 79 misclassified as negative\n",
    "```\n",
    "\n",
    "* **Negative ‚Üí Positive**: 40 cases\n",
    "* **Positive ‚Üí Negative**: 79 cases\n",
    "\n",
    "This tells us the model **struggles to distinguish negatives** from positives.\n",
    "\n",
    "\n",
    "### **3. Key Insights**\n",
    "\n",
    "* The **imbalance** (114 negatives vs 593 positives in the test set) is hurting performance.\n",
    "* The model tends to **favor positives** because they dominate the training data.\n",
    "* Business-wise: This is risky ‚ö†Ô∏è ‚Äî misclassifying a **negative review as positive** could mislead decision-makers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60165d2e",
   "metadata": {},
   "source": [
    "\n",
    "## **5. Model Building: Stage 2 (Multiclass Classification)** üìä\n",
    "\n",
    "Now we expand to the **full problem**: classifying tweets into **Positive, Negative, Neutral**.\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "1. Use the full dataset (`df_clean`) with all three sentiment labels.\n",
    "2. Train-test split (80/20, stratified to preserve balance).\n",
    "3. Build a **pipeline**:\n",
    "\n",
    "   * TF-IDF Vectorizer (custom tokenizer, unigrams + bigrams, 5000 features).\n",
    "   * Logistic Regression (multiclass setting).\n",
    "4. Evaluate with:\n",
    "\n",
    "   * **Macro F1-score** (balances performance across all 3 classes).\n",
    "   * **Classification report** (precision, recall, F1).\n",
    "   * **Confusion matrix** (to see which classes are confused).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cfc88739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Multiclass):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.31      0.51      0.38       114\n",
      "     neutral       0.74      0.67      0.71       961\n",
      "    positive       0.60      0.61      0.60       593\n",
      "\n",
      "    accuracy                           0.64      1668\n",
      "   macro avg       0.55      0.60      0.56      1668\n",
      "weighted avg       0.66      0.64      0.65      1668\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 58  37  19]\n",
      " [ 92 648 221]\n",
      " [ 40 192 361]]\n"
     ]
    }
   ],
   "source": [
    "# Full dataset (3 classes)\n",
    "X_multi = df_clean[\"tweet_text\"]\n",
    "y_multi = df_clean[\"sentiment\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "\n",
    "# Build pipeline for multiclass classification\n",
    "pipeline_multi = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        tokenizer=custom_tokenizer,\n",
    "        ngram_range=(1,2),\n",
    "        max_features=5000\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", multi_class=\"ovr\"))\n",
    "])\n",
    "\n",
    "# Train model\n",
    "pipeline_multi.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline_multi.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report (Multiclass):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
